# Car Racing - Reinforcement Learning

# Introduction

During our study year at University of Kent, we had to produce a project in a group of 4 peoples. We decided to work on a Reinforcement Learning project. The goal was to train a car to drive on a track using Reinforcement Learning algorithms. We had to implement the algorithms and the models from scratch. We also had to implement a way to visualize the training and the results of the training.

# Implemented Reinforcement algorithms
| Algorithm |Â Description | File | Applicability | Implemented ? | Responsible |
| --- | --- | --- | --- | --- | --- |
| Classic Genetic Algorithm |Â  |Genetic.py |âœ… | âœ… | Gabriel |
| Deep Q Neural Network (DQN) |Â  |DQN.py |âœ… | âœ… | Nathan |
| Neat Algorithm |Â  |NEAT.pyÂ  |âœ… | âœ… | Tom |
| DDPG Algorithm |Â  |DDPG.pyÂ  |âœ… |ðŸ”§ | Gabriel |
| PPO Algorithm |Â  |PPO.pyÂ  |âœ… | ðŸš§ | Hugo |
| Actor Critic Method |Â  |Â  |âœ… |ðŸ”´ | Hugo |
| VPG Algorithm |Â  |Â  |âœ… | ðŸš§ | Maxime |
| Q-learning or value-iteration methods |Â  |Â  |Â  |ðŸ”´ | |
| Q-Learning |Â  |Â  |Â  | ðŸ”´ | |

> ðŸ”´ : Not implemented<br>
> ðŸš§ : In progress<br>
> âœ… : Implemented

# Implemented models
| Model |Â Description | File | Implemented ? | Responsible |
| --- | --- | --- | --- | --- |
| CNN |Â Classic CNN where outputs are the value of simulation parameters |CNN.py |âœ… | Nathan |
| Fully Connected |Â Only Dense layers where outputs are the value of simulation parameters  |FullyConnected.py |âœ… | Gabriel |
| Selective CNN |Â CNN where the outputs are which move done (move predifined) |SelectiveCNN.py |âœ… | Nathan |
| Selective Fully Connected + Kmeans | Only Dense layers where the outputs are which move done (move predifined), but the input isn't the image but a single line of majoritary class selected by the Kmeans |SelectiveKMNN.py |âœ… | Nathan |

## Scheme of the models

| CNN | Fully Connected | Selective CNN | Selective Fully Connected + Kmeans |
|-----|-----------------|---------------|------------------------------------|
|<img src="img/CNN.png"/> | <img src="img/FullyConnected.png"/> | <img src="img/SelectiveCNN.png"/> | <img src="img/SelectiveKMNN.png"/> |



# Results

| Genetic Algorithm + FullyConnected |Â DQN + SelectiveKMNN|Â NEAT |Â DDPG + CNN |
|-------------------|-----|------|------|
|<img src="img/gif/FullyConnected_Genetic.gif"/> | <img src="img/gif/SelectiveKMNN_DQN.gif"/> | <img src="img/gif/Neat_NEAT.gif"/> | <img src="img/gif/CNN_DDPG.gif"/> |



# Usage

## Installation
```bash
pip install -r requirements.txt
```

## Run Training Session
```bash
python carRacing.py MODEL_NAME ALGORITHM_NAME
```

## Run a trained model
```bash
python visualize.py <MODEL_NAME> <ALGORITHM_NAME>
# or
python visualize.py <MODEL_NAME> <ALGORITHM_NAME> <SEED> # to run a specific seed
```

## View statistics of a trained model
```bash
python saves/stats.py <CSV_FILE>
```

## View statistics of with a fiter
```bash
python saves/stats.py SPECIFIC <FILTER> <TYPE_OF_STATS>
```

## View statistics of all models
```bash
python saves/stats.py ALL <TYPE_OF_STATS>
```

# Code architecture
```mermaid
flowchart TD
    A[Start] --> B[Load Brain and Estimator \nfrom entry arguments]
    B --> C{Save file\nexists ?}
    C --> |Yes| D[Load weights into brain]
    D --> E
    C --> |No| E[Loop of all simulations]
    E --> G[Simulation reset]
    G --> J[Current simulation loop]
    J --> K[Brain predicts next move]
    K --> L[Estimator memorizes current\nmove and state if needed]
    L --> M[Calculate current score\nfrom simulation]
    M --> N{Simulation\ndone ?}
    N --> |No| J
    N --> |Yes| O{New best\nscore ?}
    O --> |Yes| P[Save weights\nin file]
    O --> |No| Q
    P --> Q[Estimator updates brain's weights]
    Q --> R[Brain trains its network\nwith new weights]
    R --> E
```

<!-- # Research Papers
| Algorithm | Paper |
| --- | --- |
| Deep Q Neural Network (DQN) |Â https://medium.com/@cyberlympha/recurrent-neural-networks-in-reinforcement-learning-11600819ede4 |
| Deep Q Neural Network (DQN) |Â https://openreview.net/pdf?id=r1lyTjAqYX |
| Deep Q Neural Network (DQN) |Â https://towardsdatascience.com/deep-q-learning-tutorial-mindqn-2a4c855abffc |
| Deep Q Neural Network (DQN) |Â https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf |
| Deep Q Neural Network (DQN) |Â https://www.freecodecamp.org/news/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8 |
| Deep Q Neural Network (DQN) |Â https://keras.io/examples/rl/deep_q_network_breakout/ |
| Deep Q Neural Network (DQN) |Â https://github.com/keon/deep-q-learning/blob/master/dqn.py |
| Deep Q Neural Network (DQN) | https://huggingface.co/blog/deep-rl-dqn |
| Deep Q Neural Network (DQN) | https://huggingface.co/deep-rl-course/unit3/introduction |
| Deep Q Neural Network (DQN) | https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn_atari.py |
| Deep Q Neural Network (DQN) | https://github.com/pekaalto/DQN | -->
